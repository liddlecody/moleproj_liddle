import requests
import pandas as pd
import os

# Define the correct base URL for the ISIC API
base_url = "https://api.isic-archive.com/api/v2/images/"

# Set up parameters
limit = 100
download_count = 1000  # Target number of images to download

# Directory for storing images
output_dir = "isic_images"
os.makedirs(output_dir, exist_ok=True)

data = []
next_url = base_url + f"?limit={limit}"

while next_url and len(data) < download_count:
    response = requests.get(next_url)
    
    if response.status_code == 200:
        images_metadata = response.json()
        
        for image in images_metadata.get('results', []):
            if len(data) >= download_count:
                break

            try:
                # Extract the required metadata
                diagnosis = image['metadata']['clinical'].get('benign_malignant', 'unknown')
                if diagnosis in ['benign', 'malignant']:
                    data.append({
                        'image_id': image['isic_id'],
                        'diagnosis': diagnosis,
                        'url': image['files']['full']['url']
                    })
            except KeyError:
                continue

        # Check for pagination (next page URL)
        next_url = images_metadata.get('next', None)
    else:
        print(f"Failed to retrieve data: {response.status_code}")
        break

# Convert the data into a pandas DataFrame
df = pd.DataFrame(data)

# Download the images
for index, row in df.iterrows():
    response = requests.get(row['url'])
    if response.status_code == 200:
        with open(f"{output_dir}/{row['diagnosis']}_{row['image_id']}.jpg", 'wb') as f:
            f.write(response.content)
        print(f"Downloaded {row['diagnosis']}_{row['image_id']}.jpg")
    else:
        print(f"Failed to download image {row['image_id']}")

